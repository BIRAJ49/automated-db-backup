name: Automated PostgreSQL Backup to S3

on:
  schedule:
    - cron: "0 */12 * * *" # every 12 hours
  workflow_dispatch: # manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install PostgreSQL client and AWS CLI
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl unzip gnupg

          # Install AWS CLI v2
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install

          # Install PostgreSQL client
          sudo apt-get install -y postgresql-client

      - name: (Optional) Smoketest S3 permission
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          echo "ok" > s3-ok.txt
          aws s3 cp s3-ok.txt s3://${{ secrets.S3_BUCKET }}/postgres-backups/s3-ok.txt
          aws s3 ls s3://${{ secrets.S3_BUCKET }}/postgres-backups/

      - name: Dump PostgreSQL database
        env:
          PGPASSWORD: ${{ secrets.PG_PASSWORD }}
        run: |
          TS=$(date +'%Y-%m-%d_%H-%M-%S')
          FILE="backup_${TS}.sql.gz"
          pg_dump -h "${{ secrets.PG_HOST }}" -p "${{ secrets.PG_PORT }}" \
                  -U "${{ secrets.PG_USER }}" -d "${{ secrets.PG_DATABASE }}" \
            | gzip > "$FILE"
          echo "Created $FILE"

      - name: Upload backup to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          aws s3 cp *.sql.gz s3://${{ secrets.S3_BUCKET }}/postgres-backups/
